/***************************************************************************\
* Copyright (c) 2023, Sofie Martins                                         *   
* All rights reserved.                                                      * 
\***************************************************************************/

#include "Utils/generics.h"
#include "libhr_core.h"

#if !defined _FIELD_TYPE
#error Missing _FIELD_TYPE in sync_to_buffer_reduced.cu
#endif
#if !defined _SITE_TYPE
#error Missing _SITE_TYPE in sync_to_buffer_reduced.cu
#endif
#if !defined _GAUGE_TYPE
#error Missing _GAUGE_TYPE in sync_to_buffer_reduced.cu
#endif
#if !defined _HSPINOR_TYPE
#error Missing _SITE_TYPE in sync_to_buffer_reduced.cu
#endif
#if !defined _FIELD_DIM
#error Missing _FIELD_DIM in sync_to_buffer_reduced.cu
#endif
#if !defined _REP_SUFFIX
#error missing _REP_SUFFIX in sync_to_buffer_reduced.cu
#endif

#define _KERNEL(_name, _args) __global__ _FUNC(void, _name, _FIELD_TYPE, _args)
#define _DECLARE(_type, _name, _args) _FUNC(_type, _name, _FIELD_TYPE, _args)
#define GFIELD(_gauge, _suffix) CONCAT(_gauge, _suffix)

_KERNEL(box_to_buffer_kernel_reduced_,
        (_SITE_TYPE * in, _SITE_TYPE *out_field, _GAUGE_TYPE *gauge, coord4 *icoord, int *ipt_gpu, int vol_in, int vol_out,
         int base_in, int base_out, char mask, int *idn_gpu)) {
    for (int ix = blockIdx.x * BLOCK_SIZE + threadIdx.x; ix < vol_out; ix += gridDim.x * blockDim.x) {
        coord4 c = icoord[ix + base_out];
        int iy = ipt_ext_gpu(c.x[0], c.x[1], c.x[2], c.x[3]);
        _GAUGE_TYPE u;
        _HSPINOR_TYPE r, sn;
        _HSPINOR_TYPE *out = (_HSPINOR_TYPE *)((_SITE_TYPE *)out_field + base_out);

        if (mask & T_UP_MASK) {
            int ixg = idn_gpu[4 * iy];
            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 0);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 2);
            read_gpu<_REAL>(0, &u, gauge, ixg, 0, 4);

            _vector_add_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_T_multiply(sn.c[1], u, sn.c[0]);

            r.c[0] = sn.c[1];

            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 1);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 3);

            _vector_add_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_T_multiply(sn.c[1], u, sn.c[0]);

            r.c[1] = sn.c[1];
            write_gpu<_REAL>(0, &r, out, ix, 0, _FIELD_DIM);
        }

        if (mask & T_DN_MASK) {
            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 0);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 2);
            read_gpu<_REAL>(0, &u, gauge, iy, 0, 4);

            _vector_sub_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_T_inverse_multiply(sn.c[1], u, sn.c[0]);

            r.c[0] = sn.c[1];

            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 1);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 3);

            _vector_sub_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_T_inverse_multiply(sn.c[1], u, sn.c[0]);

            r.c[1] = sn.c[1];
            write_gpu<_REAL>(0, &r, out, ix, 0, _FIELD_DIM);
        }

        if (mask & X_UP_MASK) {
            int ixg = idn_gpu[4 * iy + 1];
            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 0);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 3);
            read_gpu<_REAL>(0, &u, gauge, ixg, 1, 4);

            _vector_i_add_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_X_multiply(sn.c[1], u, sn.c[0]);

            r.c[0] = sn.c[1];

            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 1);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 2);

            _vector_i_add_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_X_multiply(sn.c[1], u, sn.c[0]);

            r.c[1] = sn.c[1];
            write_gpu<_REAL>(0, &r, out, ix, 0, _FIELD_DIM);
        }

        if (mask & X_DN_MASK) {
            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 0);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 3);
            read_gpu<_REAL>(0, &u, gauge, iy, 1, 4);

            _vector_i_sub_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_X_inverse_multiply(sn.c[1], u, sn.c[0]);

            r.c[0] = sn.c[1];

            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 1);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 2);

            _vector_i_sub_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_X_inverse_multiply(sn.c[1], u, sn.c[0]);

            r.c[1] = sn.c[1];
            write_gpu<_REAL>(0, &r, out, ix, 0, _FIELD_DIM);
        }

        if (mask & Y_UP_MASK) {
            int ixg = idn_gpu[4 * iy + 2];
            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 0);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 3);
            read_gpu<_REAL>(0, &u, gauge, ixg, 2, 4);

            _vector_add_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_Y_multiply(sn.c[1], u, sn.c[0]);

            r.c[0] = sn.c[1];

            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 1);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 2);

            _vector_sub_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_Y_multiply(sn.c[1], u, sn.c[0]);

            r.c[1] = sn.c[1];
            write_gpu<_REAL>(0, &r, out, ix, 0, _FIELD_DIM);
        }

        if (mask & Y_DN_MASK) {
            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 0);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 3);
            read_gpu<_REAL>(0, &u, gauge, iy, 2, 4);

            _vector_sub_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_Y_inverse_multiply(sn.c[1], u, sn.c[0]);

            r.c[0] = sn.c[1];

            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 1);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 2);

            _vector_add_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_Y_inverse_multiply(sn.c[1], u, sn.c[0]);

            r.c[1] = sn.c[1];
            write_gpu<_REAL>(0, &r, out, ix, 0, _FIELD_DIM);
        }

        if (mask & Z_UP_MASK) {
            int ixg = idn_gpu[4 * iy + 3];
            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 0);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 2);
            read_gpu<_REAL>(0, &u, gauge, ixg, 3, 4);

            _vector_i_add_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_Z_multiply(sn.c[1], u, sn.c[0]);

            r.c[0] = sn.c[1];

            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 1);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 3);
            _vector_i_sub_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_Z_multiply(sn.c[1], u, sn.c[0]);

            r.c[1] = sn.c[1];
            write_gpu<_REAL>(0, &r, out, ix, 0, _FIELD_DIM);
        }

        if (mask & Z_DN_MASK) {
            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 0);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 2);
            read_gpu<_REAL>(0, &u, gauge, iy, 3, 4);

            _vector_i_sub_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_Z_inverse_multiply(sn.c[1], u, sn.c[0]);

            r.c[0] = sn.c[1];

            in_spinor_field<_REAL>(&(sn.c[0]), in, iy, 1);
            in_spinor_field<_REAL>(&(sn.c[1]), in, iy, 3);

            _vector_i_add_assign_f(sn.c[0], sn.c[1]);
            _suNf_theta_Z_inverse_multiply(sn.c[1], u, sn.c[0]);

            r.c[1] = sn.c[1];
            write_gpu<_REAL>(0, &r, out, ix, 0, _FIELD_DIM);
        }
    }
}

_DECLARE(void, sync_box_to_buffer_gpu_reduced_,
         (geometry_descriptor * gd, box_t *box, _FIELD_TYPE *in, void *sendbuf, int buf, char mask)) {
    const gd_type gd_t = gd->desc;
    const int max_vol = fmax(boxEvenVolume(box), boxOddVolume(box));
    const int grid = (max_vol - 1) / BLOCK_SIZE + 1;
    _SITE_TYPE *in_offset = in->gpu_ptr - in->type->master_shift;
    _SITE_TYPE *out_offset = (_SITE_TYPE *)sendbuf;
    _GAUGE_TYPE *gauge = GFIELD(u_gauge, _REP_SUFFIX)->gpu_ptr;

    if (gd_t & EVEN) {
        _F_NAME(box_to_buffer_kernel_reduced_, _FIELD_TYPE)<<<grid, BLOCK_SIZE, 0, memory_streams[(2 * buf) % 16]>>>(
            in_offset, out_offset, gauge, sb_icoord_gpu, ipt_gpu, boxEvenVolume(geometryBoxes), boxEvenVolume(box),
            geometryBoxes->base_index, box->base_index, mask, idn_gpu);
    }

    if (gd_t & ODD) {
        _F_NAME(box_to_buffer_kernel_reduced_, _FIELD_TYPE)<<<grid, BLOCK_SIZE, 0, memory_streams[(2 * buf + 1) % 16]>>>(
            in_offset, out_offset, gauge, sb_icoord_gpu, ipt_gpu, boxOddVolume(geometryBoxes), boxOddVolume(box),
            geometryBoxes->base_index_odd, box->base_index_odd, mask, idn_gpu);
    }
}

#undef _DECLARE
#undef _KERNEL

#undef _FIELD_TYPE
#undef _SITE_TYPE
#undef _GAUGE_TYPE
#undef _HSPINOR_TYPE
#undef _FIELD_DIM
#undef _GEOM_TYPE
#undef _COMPLEX
#undef _REAL
#undef _REP_SUFFIX