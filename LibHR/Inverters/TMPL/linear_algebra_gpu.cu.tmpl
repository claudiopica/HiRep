/***************************************************************************\
* Copyright (c) 2008,2011, Claudio Pica, Ari Hietanen and Ulrik SÃ¸ndergaard *
* All rights reserved.                                                      *
\***************************************************************************/

#include "../linear_algebra_gpu_kernels.hpp"

// Error checking
#if !defined(_SPINOR_FIELD_TYPE)
#error Missing _SPINOR_FIELD_TYPE in linear_algebra_gpu.cu
#endif
#if !defined(_SPINOR_TYPE)
#error Missing _SPINOR_TYPE in linear_algebra_gpu.cu
#endif
#if !defined(_REAL)
#error Missing _REAL in linear_algebra_gpu.cu
#endif
#if !defined(_COMPLEX)
#error Missing _COMPLEX in linear_algebra_gpu.cu
#endif
#if !defined(_SUFFIX)
#error Missing _SUFFIX in linear_algebra_gpu.cu
#endif

// Internal macro for defining generic functions and alias function pointers
#define _CONCAT(_name,_suffix) _name ## _suffix
#define _F_NAME(_name,_suffix) _CONCAT(_name,_suffix)
#define _GPU_F_NAME(_name,_suffix) _CONCAT(_name,_suffix ## _gpu)
#define _FUNC_INNER(_type,_name,_suffix,_args) _type _GPU_F_NAME(_name,_suffix) _args
#define _FUNC(_type,_name,_args) _FUNC_INNER(_type,_name,_SUFFIX,_args)

#define _TOCOMPLEX(sp) ((_COMPLEX *)(sp))
#define _NCOM (sizeof(_SPINOR_TYPE)/sizeof(_COMPLEX))


#define _CUDA_FOR(s,ixp,body) \
  do { \
    _PIECE_FOR((s)->type, (ixp)){ \
       int N = (s)->type->master_end[(ixp)] - (s)->type->master_start[(ixp)] + 1; \
       unsigned int grid_size = (N-1)/BLOCK_SIZE + 1; \
       body; \
       CudaCheckError(); \
    }	\
  } while(0)

_FUNC(void, spinor_field_copy, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  cudaMemcpy(s1->gpu_ptr,s2->gpu_ptr,s1->type->gsize_spinor*sizeof(_SPINOR_TYPE),cudaMemcpyDeviceToDevice);
}

/* auxillary functions for GPU reductions */
#ifndef ALLOC_AUX_GPU
#define ALLOC_AUX_GPU

static double* alloc_double_sum_field(int n){
  static double* res = NULL;
  static int n_size=0;
  if (n>n_size && res!=NULL){
    cudaFree(res);
    res = NULL;
  }

  if (res == NULL){
    cudaMalloc((void **) &res, n*sizeof(double));
    n_size = n;
  }
  return res;
}

static hr_complex* alloc_complex_sum_field(int n){
  static hr_complex* res = NULL;
  static int n_size = 0;
  if (n>n_size && res!=NULL){
    cudaFree(res);
    res = NULL;
  }
  if (res == NULL){
    cudaMalloc((void **) &res, n*sizeof(hr_complex));
    n_size = n;
  }
  return res;
}
#endif 

/**
* @brief Sums across GPU nodes after finding the local sum (generics)
*
* @param vector		Vector with local results
* @param size		Size of vector
*
* @return T		Result of sum of generic type T.
*/
template <class T>
T global_sum_gpu(T *vector, int size);
// This function is defined in global_sum_gpu.cu

/* Re <s1,s2> */
_FUNC(double, spinor_field_prod_re, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  double res = 0.0;
  double *resPiece;

  _CUDA_FOR(s1,ixp,
            resPiece = alloc_double_sum_field(N);
            (spinor_field_prod_re_gpu<_SPINOR_TYPE, _REAL, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),_GPU_FIELD_BLK(s2,ixp),resPiece,N));
            res += global_sum_gpu(resPiece,N);
  );
#ifdef WITH_MPI
  global_sum(&res,1);
#endif

  return res;
}

/* Im <s1,s2> */
_FUNC(double, spinor_field_prod_im, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  double res = 0.0;
  double *resPiece;

  _CUDA_FOR(s1,ixp,
            resPiece = alloc_double_sum_field(N);
            (spinor_field_prod_im_gpu<_SPINOR_TYPE, _REAL, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),_GPU_FIELD_BLK(s2,ixp),resPiece,N));
            res += global_sum_gpu(resPiece,N);
  );
#ifdef WITH_MPI
  global_sum(&res,1);
#endif

  return res;
}

/* <s1,s2> */
_FUNC(hr_complex, spinor_field_prod, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  hr_complex res = 0.0;
  hr_complex *resPiece;

  _CUDA_FOR(s1,ixp,
            resPiece = alloc_complex_sum_field(N);
            (spinor_field_prod_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp), _GPU_FIELD_BLK(s2,ixp), resPiece, N));
            res += global_sum_gpu(resPiece,N);
  );
#ifdef WITH_MPI
  global_sum((double*)&res,2);
#endif

  return res;
}

/* Re <g5*s1,s2> */
_FUNC(double, spinor_field_g5_prod_re, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  double res = 0.0;
  double *resPiece;

  _CUDA_FOR(s1,ixp, 
            resPiece = alloc_double_sum_field(N);
            (spinor_field_g5_prod_re_gpu<_SPINOR_TYPE, _COMPLEX, _REAL><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),_GPU_FIELD_BLK(s2,ixp),resPiece,N));
            res += global_sum_gpu(resPiece,N);
  );
#ifdef WITH_MPI
  global_sum(&res,1);
#endif

  return res;
}

/* Im <g5*s1,s2> */
_FUNC(double, spinor_field_g5_prod_im, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  double res = 0.0;
  double *resPiece;

  _CUDA_FOR(s1,ixp,
            resPiece = alloc_double_sum_field(N);
            (spinor_field_g5_prod_im_gpu<_SPINOR_TYPE, _COMPLEX, _REAL><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),_GPU_FIELD_BLK(s2,ixp),resPiece,N));
            res += global_sum_gpu(resPiece,N);
  );
#ifdef WITH_MPI
  global_sum(&res,1);
#endif

  return res;
}

/* Re <s1,s1> */
_FUNC(double, spinor_field_sqnorm, (_SPINOR_FIELD_TYPE *s1))
{
  double res = 0.0;
  double *resPiece;

  _CUDA_FOR(s1,ixp,
            resPiece = alloc_double_sum_field(N);
            (spinor_field_sqnorm_gpu<_SPINOR_TYPE, _COMPLEX, _REAL><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp), resPiece,N));
            res += global_sum_gpu(resPiece,N);
  );
#ifdef WITH_MPI
  global_sum(&res,1);
#endif

  return res;
}

/* s1+=r*s2 r real */
_FUNC(void, spinor_field_mul_add_assign, (_SPINOR_FIELD_TYPE *s1, _REAL r, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _CUDA_FOR(s1,ixp,
            (spinor_field_mul_add_assign_gpu<_SPINOR_TYPE, _COMPLEX, _REAL><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),r,_GPU_FIELD_BLK(s2,ixp),N));
  );
}

/* s1+=c*s2 c complex */
_FUNC(void, spinor_field_mulc_add_assign, (_SPINOR_FIELD_TYPE *s1, _COMPLEX c, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _CUDA_FOR(s1,ixp,
            (spinor_field_mulc_add_assign_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),c,_GPU_FIELD_BLK(s2,ixp),N));
  );
}

/* s1=r*s2 */
_FUNC(void, spinor_field_mul, (_SPINOR_FIELD_TYPE *s1, _REAL r, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _CUDA_FOR(s1,ixp,
            (spinor_field_mul_gpu<_SPINOR_TYPE, _COMPLEX, _REAL><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),r,_GPU_FIELD_BLK(s2,ixp),N));
  );
}

/* s1=c*s2 c complex */
_FUNC(void, spinor_field_mulc, (_SPINOR_FIELD_TYPE *s1, _COMPLEX c, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _CUDA_FOR(s1,ixp,
            (spinor_field_mulc_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),c,_GPU_FIELD_BLK(s2,ixp),N));
  );
}

/* r=s1+s2 */
_FUNC(void, spinor_field_add, (_SPINOR_FIELD_TYPE *r, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _CUDA_FOR(s1,ixp,
            (spinor_field_add_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(r,ixp),_GPU_FIELD_BLK(s1,ixp),_GPU_FIELD_BLK(s2,ixp),N));
  );
}

/* r=s1-s2 */
_FUNC(void, spinor_field_sub, (_SPINOR_FIELD_TYPE *r, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _CUDA_FOR(s1,ixp,
            (spinor_field_sub_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(r,ixp),_GPU_FIELD_BLK(s1,ixp),_GPU_FIELD_BLK(s2,ixp),N));
  );
}

/* s1+=s2 */
_FUNC(void, spinor_field_add_assign, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _CUDA_FOR(s1,ixp,
            (spinor_field_add_assign_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),_GPU_FIELD_BLK(s2,ixp),N));
  );
}

/* s1-=s2 */
_FUNC(void, spinor_field_sub_assign, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _CUDA_FOR(s1,ixp,
            (spinor_field_sub_assign_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),_GPU_FIELD_BLK(s2,ixp),N));
  );
}

/* s1=0 */
_FUNC(void, spinor_field_zero, (_SPINOR_FIELD_TYPE *s1))
{
  _CUDA_FOR(s1,ixp,
            (spinor_field_zero_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),N));
  );
}

/* s1=-s2 */
_FUNC(void, spinor_field_minus, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _CUDA_FOR(s1,ixp,
            (spinor_field_minus_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),_GPU_FIELD_BLK(s2,ixp),N));
  );
}

/* s1=-s1 */
_FUNC(void, spinor_field_minus_assign, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _CUDA_FOR(s1,ixp,
            (spinor_field_minus_assign_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp), N));
  );
}

/* s1=r1*s2+r2*s3 */
_FUNC(void, spinor_field_lc, (_SPINOR_FIELD_TYPE *s1, _REAL r1, _SPINOR_FIELD_TYPE *s2, _REAL r2, _SPINOR_FIELD_TYPE *s3))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _TWO_SPINORS_MATCHING(s1,s3);
  _CUDA_FOR(s1,ixp,
            (spinor_field_lc_gpu<_SPINOR_TYPE, _COMPLEX, _REAL><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),r1,_GPU_FIELD_BLK(s2,ixp),r2,_GPU_FIELD_BLK(s3,ixp),N));
  );
}

/* s1+=r1*s2+r2*s3 */
_FUNC(void, spinor_field_lc_add_assign, (_SPINOR_FIELD_TYPE *s1, _REAL r1, _SPINOR_FIELD_TYPE *s2, _REAL r2, _SPINOR_FIELD_TYPE *s3))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _TWO_SPINORS_MATCHING(s1,s3);
  _CUDA_FOR(s1,ixp,
            (spinor_field_lc_add_assign_gpu<_SPINOR_TYPE, _COMPLEX, _REAL><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),r1,_GPU_FIELD_BLK(s2,ixp),r2,_GPU_FIELD_BLK(s3,ixp),N));
  );
}

/* s1=c1*s2+c2*s3 c1, c2 complex*/
_FUNC(void, spinor_field_clc, (_SPINOR_FIELD_TYPE *s1, _COMPLEX c1, _SPINOR_FIELD_TYPE *s2, _COMPLEX c2, _SPINOR_FIELD_TYPE *s3))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _TWO_SPINORS_MATCHING(s1,s3);
  _CUDA_FOR(s1,ixp,
            (spinor_field_clc_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),c1,_GPU_FIELD_BLK(s2,ixp),c2,_GPU_FIELD_BLK(s3,ixp),N));
  );
}

/* s1+=c1*s2+c2*s3 c1, c2 complex*/
_FUNC(void, spinor_field_clc_add_assign, (_SPINOR_FIELD_TYPE *s1, _COMPLEX c1, _SPINOR_FIELD_TYPE *s2, _COMPLEX c2, _SPINOR_FIELD_TYPE *s3))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _TWO_SPINORS_MATCHING(s1,s3);
  _CUDA_FOR(s1,ixp,
            (spinor_field_clc_add_assign_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),c1,_GPU_FIELD_BLK(s2,ixp),c2,_GPU_FIELD_BLK(s3,ixp),N));
  );
}

/* s1=g5*s2  */
_FUNC(void, spinor_field_g5, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _CUDA_FOR(s1,ixp,
             (spinor_field_g5_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),_GPU_FIELD_BLK(s2,ixp),N))
  );
}

/* s1=g5*s1  */
_FUNC(void, spinor_field_g5_assign, (_SPINOR_FIELD_TYPE *s1))
{
  _CUDA_FOR(s1,ixp,
             (spinor_field_g5_assign_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),N))
  );
}

/* s1+=c*g5*s2 c complex */
_FUNC(void, spinor_field_g5_mulc_add_assign, (_SPINOR_FIELD_TYPE *s1, _COMPLEX c, _SPINOR_FIELD_TYPE *s2))
{
   _TWO_SPINORS_MATCHING(s1,s2);
   _CUDA_FOR(s1,ixp,
            (spinor_field_g5_mulc_add_assign_gpu<_SPINOR_TYPE, _COMPLEX><<<grid_size,BLOCK_SIZE>>>(_GPU_FIELD_BLK(s1,ixp),c,_GPU_FIELD_BLK(s2,ixp),N))
   );
}

/* tools per eva.c  */
_FUNC(void, spinor_field_lc1, (_REAL c1, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _CUDA_FOR(s1,ixp,
            (spinor_field_lc1_gpu<_SPINOR_TYPE, _COMPLEX, _REAL><<<grid_size,BLOCK_SIZE>>>(c1,_GPU_FIELD_BLK(s1,ixp),_GPU_FIELD_BLK(s2,ixp),N));
  );
}

_FUNC(void, spinor_field_lc2, (_REAL c1, _REAL c2, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  _CUDA_FOR(s1,ixp,
            (spinor_field_lc2_gpu<_SPINOR_TYPE, _COMPLEX, _REAL><<<grid_size,BLOCK_SIZE>>>(c1,c2,_GPU_FIELD_BLK(s1,ixp),_GPU_FIELD_BLK(s2,ixp),N));
  );
}

_FUNC(void, spinor_field_lc3, (_REAL c1, _REAL c2, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2, _SPINOR_FIELD_TYPE *s3))
{
  _TWO_SPINORS_MATCHING(s1,s2);
  /* c1=-c1; c2=-c2; */
  _REAL cc1=-c1;
  _REAL cc2=-c2;
  _CUDA_FOR(s1,ixp,
            (spinor_field_lc3_gpu<_SPINOR_TYPE, _COMPLEX, _REAL><<<grid_size,BLOCK_SIZE>>>(cc1,cc2,_GPU_FIELD_BLK(s1,ixp),_GPU_FIELD_BLK(s2,ixp),_GPU_FIELD_BLK(s3,ixp),N));
  );
}

/* Placeholder functions, there need to be the same definitions in linear_algebra.c.sdtmpl and linear_algebra_gpu.c.sdtmpl */
_FUNC(void, spinor_field_g0, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{}
_FUNC(void, spinor_field_g1, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{}
_FUNC(void, spinor_field_g2, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{}
_FUNC(void, spinor_field_g3, (_SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2))
{}


#undef _TOCOMPLEX
#undef _NCOM

//undefine macros
//internal macros
#undef _CONCAT
#undef _F_NAME
#undef _GPU_F_NAME
#undef _FUNC_INNER
#undef _FUNC

//user interface macros
#undef _SPINOR_FIELD_TYPE
#undef _SPINOR_TYPE
#undef _REAL
#undef _COMPLEX
#undef _SUFFIX

