/***************************************************************************\
* Copyright (c) 2008-2023                                                   *
* Claudio Pica, Ari Hietanen, Ulrik SÃ¸ndergaard, Sofie Martins              *
* All rights reserved.                                                      *
\***************************************************************************/

#include "../linear_algebra_gpu_kernels.hpp"

// Error checking
#if !defined(_SPINOR_FIELD_TYPE)
#error Missing _SPINOR_FIELD_TYPE in linear_algebra_gpu.cu
#endif
#if !defined(_SPINOR_TYPE)
#error Missing _SPINOR_TYPE in linear_algebra_gpu.cu
#endif
#if !defined(_REAL)
#error Missing _REAL in linear_algebra_gpu.cu
#endif
#if !defined(_COMPLEX)
#error Missing _COMPLEX in linear_algebra_gpu.cu
#endif
#if !defined(_SUFFIX)
#error Missing _SUFFIX in linear_algebra_gpu.cu
#endif

// Internal macro for defining generic functions and alias function pointers
#define _DECLARE(_type, _name, _args) _GPU_FUNC(_type, _name, _SUFFIX, _args)

#define _TOCOMPLEX(sp) ((_COMPLEX *)(sp))
#define _NCOM (sizeof(_SPINOR_TYPE) / sizeof(_COMPLEX))

#define _CUDA_FOR(s, ixp, body)                                                        \
    do {                                                                               \
        _PIECE_FOR((s)->type, (ixp)) {                                                 \
            int N = (s)->type->master_end[(ixp)] - (s)->type->master_start[(ixp)] + 1; \
            unsigned int grid_size = (N - 1) / BLOCK_SIZE_LINEAR_ALGEBRA + 1;          \
            body;                                                                      \
            CudaCheckError();                                                          \
        }                                                                              \
    } while (0)

_DECLARE(void, spinor_field_copy, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    cudaMemcpy(s1->gpu_ptr, s2->gpu_ptr, s1->type->gsize_spinor * sizeof(_SPINOR_TYPE), cudaMemcpyDeviceToDevice);
}

/**
* @brief Sums across GPU nodes after finding the local sum (generics)
*
* @param vector		Vector with local results
* @param size		Size of vector
*
* @return T		Result of sum of generic type T.
*/
template <class T> T global_sum_gpu(T *vector, int size);
// This function is defined in global_sum_gpu.cu

/* Re <s1,s2> */
_DECLARE(double, spinor_field_prod_re, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    double res = 0.0;
    double *resPiece;

    _CUDA_FOR(s1, ixp, resPiece = alloc_double_sum_field(N);
              (spinor_field_prod_re_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), resPiece, N));
              res += global_sum_gpu(resPiece, N););
#ifdef WITH_MPI
    global_sum(&res, 1);
#endif

    return res;
}

/* Im <s1,s2> */
_DECLARE(double, spinor_field_prod_im, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    double res = 0.0;
    double *resPiece;

    _CUDA_FOR(s1, ixp, resPiece = alloc_double_sum_field(N);
              (spinor_field_prod_im_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), resPiece, N));
              res += global_sum_gpu(resPiece, N););
#ifdef WITH_MPI
    global_sum(&res, 1);
#endif

    return res;
}

/* <s1,s2> */
_DECLARE(hr_complex, spinor_field_prod, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    hr_complex res = 0.0;
    hr_complex *resPiece;

    _CUDA_FOR(s1, ixp, resPiece = alloc_complex_sum_field(N);
              (spinor_field_prod_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), resPiece, N));
              res += global_sum_gpu(resPiece, N););
#ifdef WITH_MPI
    global_sum((double *)&res, 2);
#endif

    return res;
}

/* Re <g5*s1,s2> */
_DECLARE(double, spinor_field_g5_prod_re, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    double res = 0.0;
    double *resPiece;

    _CUDA_FOR(s1, ixp, resPiece = alloc_double_sum_field(N);
              (spinor_field_g5_prod_re_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), resPiece, N));
              res += global_sum_gpu(resPiece, N););
#ifdef WITH_MPI
    global_sum(&res, 1);
#endif

    return res;
}

/* Im <g5*s1,s2> */
_DECLARE(double, spinor_field_g5_prod_im, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    double res = 0.0;
    double *resPiece;

    _CUDA_FOR(s1, ixp, resPiece = alloc_double_sum_field(N);
              (spinor_field_g5_prod_im_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), resPiece, N));
              res += global_sum_gpu(resPiece, N););
#ifdef WITH_MPI
    global_sum(&res, 1);
#endif

    return res;
}

/* Re <s1,s1> */
_DECLARE(double, spinor_field_sqnorm, (_SPINOR_FIELD_TYPE * s1)) {
    double res = 0.0;
    double *resPiece;

    _CUDA_FOR(s1, ixp, resPiece = alloc_double_sum_field(N);
              (spinor_field_sqnorm_gpu<_REAL><<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), resPiece, N));
              res += global_sum_gpu(resPiece, N););
#ifdef WITH_MPI
    global_sum(&res, 1);
#endif

    return res;
}

/* s1+=r*s2 r real */
_DECLARE(void, spinor_field_mul_add_assign, (_SPINOR_FIELD_TYPE * s1, _REAL r, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _CUDA_FOR(s1, ixp,
              (spinor_field_mul_add_assign_gpu<<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), r,
                                                                                         _GPU_FIELD_BLK(s2, ixp), N)););
}

/* s1+=c*s2 c complex */
_DECLARE(void, spinor_field_mulc_add_assign, (_SPINOR_FIELD_TYPE * s1, _COMPLEX c, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _CUDA_FOR(s1, ixp,
              (spinor_field_mulc_add_assign_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), c, _GPU_FIELD_BLK(s2, ixp), N)););
}

/* s1+=c*g5*s2 c complex */
_DECLARE(void, spinor_field_g5_mulc_add_assign, (_SPINOR_FIELD_TYPE * s1, _COMPLEX c, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _CUDA_FOR(s1, ixp,
              (spinor_field_g5_mulc_add_assign_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), c, _GPU_FIELD_BLK(s2, ixp), N)));
}

/* s1=r*s2 */
_DECLARE(void, spinor_field_mul, (_SPINOR_FIELD_TYPE * s1, _REAL r, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _CUDA_FOR(s1, ixp,
              (spinor_field_mul_gpu<<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), r,
                                                                              _GPU_FIELD_BLK(s2, ixp), N)););
}

/* s1=c*s2 c complex */
_DECLARE(void, spinor_field_mulc, (_SPINOR_FIELD_TYPE * s1, _COMPLEX c, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _CUDA_FOR(s1, ixp,
              (spinor_field_mulc_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), c, _GPU_FIELD_BLK(s2, ixp), N)););
}

/* r=s1+s2 */
_DECLARE(void, spinor_field_add, (_SPINOR_FIELD_TYPE * r, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _CUDA_FOR(s1, ixp,
              (spinor_field_add_gpu<_REAL><<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(
                  _GPU_FIELD_BLK(r, ixp), _GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), N)););
}

/* r=s1-s2 */
_DECLARE(void, spinor_field_sub, (_SPINOR_FIELD_TYPE * r, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _CUDA_FOR(s1, ixp,
              (spinor_field_sub_gpu<_REAL><<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(
                  _GPU_FIELD_BLK(r, ixp), _GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), N)););
}

/* s1+=s2 */
_DECLARE(void, spinor_field_add_assign, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _CUDA_FOR(s1, ixp,
              (spinor_field_add_assign_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), N)););
}

/* s1-=s2 */
_DECLARE(void, spinor_field_sub_assign, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _CUDA_FOR(s1, ixp,
              (spinor_field_sub_assign_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), N)););
}

/* s1=0 */
_DECLARE(void, spinor_field_zero, (_SPINOR_FIELD_TYPE * s1)) {
    _CUDA_FOR(s1, ixp, (spinor_field_zero_gpu<_REAL><<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), N)););
}

/* s1=-s2 */
_DECLARE(void, spinor_field_minus, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _CUDA_FOR(s1, ixp,
              (spinor_field_minus_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), N)););
}

/* s1=r1*s2+r2*s3 */
_DECLARE(void, spinor_field_lc, (_SPINOR_FIELD_TYPE * s1, _REAL r1, _SPINOR_FIELD_TYPE *s2, _REAL r2, _SPINOR_FIELD_TYPE *s3)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _TWO_SPINORS_MATCHING(s1, s3);
    _CUDA_FOR(s1, ixp,
              (spinor_field_lc_gpu<<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(
                  _GPU_FIELD_BLK(s1, ixp), r1, _GPU_FIELD_BLK(s2, ixp), r2, _GPU_FIELD_BLK(s3, ixp), N)););
}

/* s1+=r1*s2+r2*s3 */
_DECLARE(void, spinor_field_lc_add_assign,
         (_SPINOR_FIELD_TYPE * s1, _REAL r1, _SPINOR_FIELD_TYPE *s2, _REAL r2, _SPINOR_FIELD_TYPE *s3)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _TWO_SPINORS_MATCHING(s1, s3);
    _CUDA_FOR(s1, ixp,
              (spinor_field_lc_add_assign_gpu<<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(
                  _GPU_FIELD_BLK(s1, ixp), r1, _GPU_FIELD_BLK(s2, ixp), r2, _GPU_FIELD_BLK(s3, ixp), N)););
}

/* s1=c1*s2+c2*s3 c1, c2 complex*/
_DECLARE(void, spinor_field_clc,
         (_SPINOR_FIELD_TYPE * s1, _COMPLEX c1, _SPINOR_FIELD_TYPE *s2, _COMPLEX c2, _SPINOR_FIELD_TYPE *s3)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _TWO_SPINORS_MATCHING(s1, s3);
    _CUDA_FOR(s1, ixp,
              (spinor_field_clc_gpu<_REAL><<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(
                  _GPU_FIELD_BLK(s1, ixp), c1, _GPU_FIELD_BLK(s2, ixp), c2, _GPU_FIELD_BLK(s3, ixp), N)););
}

/* s1+=c1*s2+c2*s3 c1, c2 complex*/
_DECLARE(void, spinor_field_clc_add_assign,
         (_SPINOR_FIELD_TYPE * s1, _COMPLEX c1, _SPINOR_FIELD_TYPE *s2, _COMPLEX c2, _SPINOR_FIELD_TYPE *s3)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _TWO_SPINORS_MATCHING(s1, s3);
    _CUDA_FOR(s1, ixp,
              (spinor_field_clc_add_assign_gpu<_REAL><<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(
                  _GPU_FIELD_BLK(s1, ixp), c1, _GPU_FIELD_BLK(s2, ixp), c2, _GPU_FIELD_BLK(s3, ixp), N)););
}

/* s1=g5*s2  */
_DECLARE(void, spinor_field_g5, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _CUDA_FOR(s1, ixp,
              (spinor_field_g5_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), N)));
}

/* s1+=g5*s1  */
_DECLARE(void, spinor_field_g5_assign, (_SPINOR_FIELD_TYPE * s1)) {
    _CUDA_FOR(s1, ixp,
              (spinor_field_g5_assign_gpu<_REAL><<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), N)));
}

_DECLARE(void, spinor_field_g0, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _CUDA_FOR(s1, ixp,
              (spinor_field_g0_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), N)));
}

_DECLARE(void, spinor_field_g1, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _CUDA_FOR(s1, ixp,
              (spinor_field_g1_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), N)));
}

_DECLARE(void, spinor_field_g2, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _CUDA_FOR(s1, ixp,
              (spinor_field_g2_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), N)));
}

_DECLARE(void, spinor_field_g3, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _CUDA_FOR(s1, ixp,
              (spinor_field_g3_gpu<_REAL>
               <<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), N)));
}

/* tools per eva.c  */
_DECLARE(void, spinor_field_lc1, (_REAL c1, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _CUDA_FOR(s1, ixp,
              (spinor_field_lc1_gpu<<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(c1, _GPU_FIELD_BLK(s1, ixp),
                                                                              _GPU_FIELD_BLK(s2, ixp), N)););
}

_DECLARE(void, spinor_field_lc2, (_REAL c1, _REAL c2, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    _CUDA_FOR(s1, ixp,
              (spinor_field_lc2_gpu<<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(c1, c2, _GPU_FIELD_BLK(s1, ixp),
                                                                              _GPU_FIELD_BLK(s2, ixp), N)););
}

_DECLARE(void, spinor_field_lc3, (_REAL c1, _REAL c2, _SPINOR_FIELD_TYPE *s1, _SPINOR_FIELD_TYPE *s2, _SPINOR_FIELD_TYPE *s3)) {
    _TWO_SPINORS_MATCHING(s1, s2);
    /* c1=-c1; c2=-c2; */
    _REAL cc1 = -c1;
    _REAL cc2 = -c2;
    _CUDA_FOR(s1, ixp,
              (spinor_field_lc3_gpu<<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(
                  cc1, cc2, _GPU_FIELD_BLK(s1, ixp), _GPU_FIELD_BLK(s2, ixp), _GPU_FIELD_BLK(s3, ixp), N)););
}

// Extra
/* s1=-s1 */
_DECLARE(void, spinor_field_minus_assign, (_SPINOR_FIELD_TYPE * s1, _SPINOR_FIELD_TYPE *s2)) {
    _CUDA_FOR(s1, ixp,
              (spinor_field_minus_assign_gpu<_REAL><<<grid_size, BLOCK_SIZE_LINEAR_ALGEBRA>>>(_GPU_FIELD_BLK(s1, ixp), N)););
}

//internal macros
#undef _DECLARE
#undef _TOCOMPLEX
#undef _NCOM

//user interface macros
#undef _SPINOR_FIELD_TYPE
#undef _SPINOR_TYPE
#undef _REAL
#undef _COMPLEX
#undef _SUFFIX
